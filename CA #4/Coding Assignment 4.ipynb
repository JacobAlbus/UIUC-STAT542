{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d34e894f-382a-40d2-82be-4d8c5d2a5cef",
   "metadata": {},
   "source": [
    "Jacob Albus (albus2 campus) Ashish Pabba (apabba2 MCS-DS) Amarthya Kuchana (kuchana2 campus)\n",
    "\n",
    "We each worked independently then brought our code together, helping each other with holes when they appeared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d9b3bd-de61-497d-8907-669097d1a1bd",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e018ea7c-d9ed-4be2-bb15-9495c818108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "faithful = pd.read_csv('data/faithful.dat', delimiter='\\t')\n",
    "\n",
    "# When reading in .dat file, the two columns are merged into one. Below code fixes this\n",
    "eruptions = []\n",
    "waitings = []\n",
    "for index, row in enumerate(faithful.iloc[:]['    eruptions waiting']):\n",
    "    try:\n",
    "        index, eruption_len, waiting_len = row.split('      ')        \n",
    "    except ValueError:\n",
    "        index, eruption_len, waiting_len = row.split('     ')\n",
    "\n",
    "    eruptions.append(float(eruption_len))\n",
    "    waitings.append(int(waiting_len))\n",
    "\n",
    "faithful[\"eruptions\"] = eruptions\n",
    "faithful[\"waitings\"] = waitings\n",
    "faithful = faithful.drop(columns=['    eruptions waiting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fb4f711-49f8-4ebc-bddd-6697cd3f161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EStep(G, means, variances, data, mixing_probs):\n",
    "    data_count, feature_count = data.shape\n",
    "    \n",
    "    # reshape to have G copies of original data\n",
    "    data = np.repeat(data.reshape((1, data_count, feature_count)), G, axis=0)\n",
    "    # reshape means to have same dimensions as updated data\n",
    "    means = np.repeat(means.reshape((G, 1, feature_count)), data_count, axis=1)\n",
    "\n",
    "    centered_data = data - means\n",
    "    determinant = np.linalg.det(variances)\n",
    "    covariance_inv = np.linalg.inv(variances)\n",
    "    \n",
    "    normalized_data = (centered_data * (covariance_inv @ centered_data.swapaxes(1,2)).swapaxes(1, 2)) / -2\n",
    "    normalized_data = np.sum(normalized_data, axis=2)\n",
    "\n",
    "    responsibilities = np.exp(normalized_data) / np.sqrt(determinant * np.power(2 * np.pi, feature_count))\n",
    "    responsibilities = responsibilities.T * mixing_probs\n",
    "\n",
    "    denominator = np.sum(responsibilities, axis=1)[:, None]\n",
    "    \n",
    "    return responsibilities / denominator\n",
    "\n",
    "def MStep(G, responsibilities, data):\n",
    "    data_count, feature_count = data.shape\n",
    "    effective_counts = np.sum(responsibilities, axis=0)\n",
    "\n",
    "    N = data.shape[0]\n",
    "    mixing_probs = effective_counts / N\n",
    "    \n",
    "    # reshape to have G copies of original data\n",
    "    data = np.repeat(data.reshape((1, data_count, feature_count)), G, axis=0)\n",
    "    # reshape responsibilities to have same dimensions as updated data\n",
    "    responsibilities = responsibilities.T.reshape(G, data_count, 1)\n",
    "    \n",
    "    means = (np.sum(responsibilities * data, axis=1).T / effective_counts).T\n",
    "\n",
    "    # reshape means to have same dimensions as updated data\n",
    "    centered_data = data - np.repeat(means.reshape((G, 1, feature_count)), data_count, axis=1)\n",
    "    variances = (responsibilities * centered_data).swapaxes(1, 2) @ centered_data\n",
    "\n",
    "    # Sum covariance matrix for each group\n",
    "    variances = np.sum(variances, axis=0) / data_count\n",
    "    \n",
    "    return means, variances, mixing_probs\n",
    "\n",
    "def loglik(G, data, means, variances, mixing_probs):\n",
    "    data_count, feature_count = data.shape\n",
    "    \n",
    "    # reshape to have G copies of original data\n",
    "    data = np.repeat(data.reshape((1, data_count, feature_count)), G, axis=0)\n",
    "    # reshape means to have same dimensions as updated data\n",
    "    means = np.repeat(means.reshape((G, 1, feature_count)), data_count, axis=1)\n",
    "\n",
    "    centered_data = data - means\n",
    "    determinant = np.linalg.det(variances)\n",
    "    covariance_inv = np.linalg.inv(variances)\n",
    "    \n",
    "    normalized_data = (centered_data * (covariance_inv @ centered_data.swapaxes(1,2)).swapaxes(1, 2)) / -2\n",
    "    normalized_data = np.sum(normalized_data, axis=2)\n",
    "    normalized_data = np.exp(normalized_data) / np.sqrt(determinant * np.power(2 * np.pi, feature_count))\n",
    "\n",
    "    mixed_data = np.sum(normalized_data.T * mixing_probs, axis=1)\n",
    "    \n",
    "    return np.sum(np.log(mixed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c09d9d3-1e08-4157-af76-8ab1c8205326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myEM(data, G, itmax, means, variances, mixing_probs):\n",
    "\n",
    "    for i in range(itmax):\n",
    "        responsibilities = EStep(G, means, variances, data, mixing_probs)\n",
    "        means, variances, mixing_probs = MStep(G, responsibilities, data)\n",
    "\n",
    "    loglikelihood = loglik(G, data, means, variances, mixing_probs)\n",
    "    \n",
    "    return mixing_probs, means, variances, loglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e89e3-b155-48e0-84c6-f9cf5c5bee4c",
   "metadata": {},
   "source": [
    "### 2 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "102dd409-0a5f-49e6-a9f8-912581f7543e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04297883 0.95702117]\n",
      "[[ 3.49564188  3.48743016]\n",
      " [76.79789154 70.63205853]]\n",
      "[[  1.29793612  13.92433626]\n",
      " [ 13.92433626 182.58009247]]\n",
      "-1289.5693549424107\n"
     ]
    }
   ],
   "source": [
    "data = faithful.to_numpy()\n",
    "n = data.shape[0]\n",
    "\n",
    "first_group_size = 10\n",
    "first_group = data[:first_group_size]\n",
    "second_group = data[first_group_size:]\n",
    "\n",
    "means = np.array([first_group.mean(axis=0), second_group.mean(axis=0)])\n",
    "\n",
    "centered1 = first_group - means[0]\n",
    "centered2 = second_group - means[1]\n",
    "covariance = ((centered1.T @ centered1) + (centered2.T @ centered2)) / n\n",
    "\n",
    "mixing_probs = np.array([first_group_size / n, 1 - (first_group_size / n)])\n",
    "\n",
    "mixing_probs, means, variances, loglikelihood = myEM(data, 2, 20, means, covariance, mixing_probs)\n",
    "print(mixing_probs)\n",
    "print(means.T)\n",
    "print(variances)\n",
    "print(loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d295f8d4-3f74-4bb9-8b3a-6361abf96a14",
   "metadata": {},
   "source": [
    "### 3 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9839c92f-176c-42a0-9c62-124fa9cce7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04363422 0.07718656 0.87917922]\n",
      "[[ 3.51006918  2.81616674  3.54564083]\n",
      " [77.10563811 63.35752634 71.25084801]]\n",
      "[[  1.26015772  13.51153756]\n",
      " [ 13.51153756 177.96419105]]\n",
      "-1289.3509588627387\n"
     ]
    }
   ],
   "source": [
    "data = faithful.to_numpy()\n",
    "n = data.shape[0]\n",
    "\n",
    "first_group_size = 10\n",
    "second_group_size = 20\n",
    "\n",
    "first_group = data[:first_group_size]\n",
    "second_group = data[first_group_size: (first_group_size + second_group_size)]\n",
    "third_group = data[(first_group_size + second_group_size):]\n",
    "\n",
    "means = np.array([first_group.mean(axis=0), second_group.mean(axis=0), third_group.mean(axis=0)])\n",
    "\n",
    "centered1 = first_group - means[0]\n",
    "centered2 = second_group - means[1]\n",
    "centered3 = third_group - means[2]\n",
    "covariance = ((centered1.T @ centered1) + (centered2.T @ centered2) + (centered3.T @ centered3)) / n\n",
    "\n",
    "mixing_probs = [first_group_size / n, second_group_size / n]\n",
    "mixing_probs.append(1 - sum(mixing_probs))\n",
    "mixing_probs = np.array(mixing_probs)\n",
    "\n",
    "mixing_probs, means, variances, loglikelihood = myEM(data, 3, 20, means, covariance, mixing_probs)\n",
    "print(mixing_probs)\n",
    "print(means.T)\n",
    "print(variances)\n",
    "print(loglikelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41883e19-f8ed-4c0c-92eb-4ed6f38dded6",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd232da0-6fab-439e-9018-ce9e6fed2363",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"data/coding4_part2_data.txt\").astype(int)\n",
    "\n",
    "data_z = []\n",
    "with open(\"data/Coding4_part2_Z.txt\") as file:\n",
    "    for line in file:\n",
    "        \n",
    "        numbers = line.split(\" \")\n",
    "        numbers[-1] = numbers[-1][:-1] # remove '\\n' character from end of line\n",
    "        numbers = [int(num) for num in numbers]\n",
    "        data_z.extend(numbers)\n",
    "        \n",
    "data_z = np.array(data_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44605f0e-f3fb-4460-9983-b588e7d023af",
   "metadata": {},
   "source": [
    "### Baum-Welch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "754ac6f4-f08c-4bb5-ab4c-c4de6dbd0edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_probabilities(w, A, B, data):\n",
    "    mz = B.shape[0]\n",
    "    N = data.shape[0]\n",
    "    \n",
    "    probs = np.zeros((N, mz))\n",
    "    probs[0] = w[0] * B[:, data[0] - 1]\n",
    "\n",
    "    for t in range(1, N):\n",
    "        probs[t] = B[:, data[t] - 1] * np.sum(A.T * probs[t - 1], axis=1)\n",
    "\n",
    "    return probs\n",
    "\n",
    "def backward_probabilities(w, A, B, data):\n",
    "    mz = B.shape[0]\n",
    "    N = data.shape[0]\n",
    "    \n",
    "    probs = np.zeros((N, mz))\n",
    "    probs[N - 1] = 1\n",
    "    for t in range(N - 1, 0, -1):\n",
    "        probs[t - 1] = np.sum(A * probs[t] * B[:, data[t] - 1], axis=1)\n",
    "\n",
    "    return probs\n",
    "\n",
    "def calculate_gammas(w, A, B, data):\n",
    "    forward_probs = forward_probabilities(w, A, B, data)\n",
    "    backwards_probs = backward_probabilities(w, A, B, data)\n",
    "\n",
    "    mz = B.shape[0]\n",
    "    N = data.shape[0]\n",
    "    gammas = np.zeros((N - 1, mz, mz))\n",
    "\n",
    "    for t in range(N - 1):\n",
    "        gammas[t] = (A * B[:, data[t + 1] - 1] * backwards_probs[t + 1]).T * forward_probs[t]\n",
    "        gammas[t] = gammas[t].T\n",
    "        gammas[t] /= np.sum(gammas[t])\n",
    "\n",
    "    return gammas\n",
    "\n",
    "def EM(w, A, B, data, num_iterations):\n",
    "    unique_data = np.unique(data)\n",
    "    \n",
    "    for i in range(num_iterations):        \n",
    "        gammas = calculate_gammas(w, A, B, data)\n",
    "        gammas_sum = np.sum(gammas, axis=2)\n",
    "\n",
    "        last_row = np.sum(gammas[-1], axis=0).reshape((1, -1))\n",
    "        gammas_sum = np.append(gammas_sum, last_row, axis=0)\n",
    "\n",
    "        A = np.sum(gammas, axis=0)\n",
    "        A = A.T / np.sum(A, axis=1)\n",
    "        A = A.T\n",
    "\n",
    "        denominator = np.sum(gammas_sum, axis=0)\n",
    "        for l in unique_data:\n",
    "            indices = np.where(data == l)\n",
    "            numerator = np.sum(gammas_sum[indices], axis=0)\n",
    "            B[:, (l - 1)] = numerator / denominator\n",
    "\n",
    "    return w, A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36e18e7b-0c6d-4f8d-82e3-2489b28680b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49793938 0.50206062]\n",
      " [0.44883431 0.55116569]]\n",
      "[[0.22159897 0.20266127 0.57573976]\n",
      " [0.34175148 0.17866665 0.47958186]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([0.5, 0.5])\n",
    "A = np.array([[0.5, 0.5], [0.5, 0.5]])\n",
    "B = np.array([[1/9, 3/9, 5/9], [1/6, 2/6, 3/6]])\n",
    "\n",
    "w, A, B = EM(w, A, B, data, 100)\n",
    "print(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c4b664-4de2-4b2e-9b05-1cdd31bce57a",
   "metadata": {},
   "source": [
    "### Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ff9023b-11e4-4a89-a186-222de16d6f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 2 1 1 1 1 1 2 2 1 1 1 1 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1 2 1 1\n",
      " 1 1 1 1 1 1 2 2 1 1 1 1 1 1 2 2 2 1 1 1 1 2 2 2 2 1 1 1 1 1 1 1 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1\n",
      " 1 1 1 2 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2\n",
      " 2 2 2 1 1 1 2 2 2 2 2 2 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 1 1 2 2 2 1 1 1 1 1\n",
      " 1 1 1 2 2 2 2 2 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "def viterbi(w, A, B, data):\n",
    "    mz = B.shape[0]\n",
    "    N = data.shape[0]\n",
    "    \n",
    "    delta = np.zeros((N, mz))\n",
    "    delta[0] = w * B[:, data[0]]\n",
    "\n",
    "    for t in range(1, N):\n",
    "        delta[t] = np.max(delta[t - 1] * A.T, axis=1) * B[:, data[t] - 1]\n",
    "\n",
    "    best_path = np.zeros(N).astype(int)\n",
    "    best_path[-1] = int(np.argmax(delta[-1]))\n",
    "    \n",
    "    for t in range(N - 2, -1, -1):\n",
    "        best_path[t] = np.argmax(delta[t] * A[:, best_path[t+1]])\n",
    "\n",
    "    return best_path + 1\n",
    "\n",
    "best_path = viterbi(w, A, B, data)\n",
    "print(best_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "258caaed-969e-4f5b-899f-6bdf137b568d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "is_right = True\n",
    "for z1, z2 in zip(best_path, data_z):\n",
    "    if z1 != z2:\n",
    "        is_right = False\n",
    "print(is_right) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
