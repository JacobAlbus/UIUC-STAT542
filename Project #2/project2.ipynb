{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52e6ee35-4e51-41c2-9391-c1ca030a37ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import patsy\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess(data):\n",
    "    tmp = pd.to_datetime(data['Date'])\n",
    "    data['Wk'] = tmp.dt.isocalendar().week\n",
    "    data['Yr'] = tmp.dt.year\n",
    "    data['Wk'] = pd.Categorical(data['Wk'], categories=[i for i in range(1, 53)])  # 52 weeks \n",
    "    return data\n",
    "\n",
    "def PCATransform(train, d=8):\n",
    "    train_svd = pd.DataFrame()\n",
    "\n",
    "    for dept in train[\"Dept\"].unique():\n",
    "        filtered_train = train[train['Dept'] == dept]\n",
    "        selected_columns = filtered_train[['Store', 'Date', 'Weekly_Sales']]\n",
    "        \n",
    "        train_dept_ts = selected_columns.pivot(index='Date', columns='Store', values='Weekly_Sales').reset_index().T\n",
    "        train_dept_ts = train_dept_ts.fillna(0)\n",
    "\n",
    "        sales_data = train_dept_ts.iloc[1:].to_numpy()\n",
    "        store_mean = np.mean(sales_data, axis=1)\n",
    "        centered_data = (sales_data.T - store_mean).T\n",
    "        centered_data = centered_data.astype(float)\n",
    "\n",
    "        try:\n",
    "            U, S, Vh = np.linalg.svd(centered_data)\n",
    "            S = np.diag(S)\n",
    "            smooth_sales = np.dot(U[:, :d], np.dot(S[:d, :d], Vh[:d, :]))\n",
    "            smooth_sales = (smooth_sales.T + store_mean).T\n",
    "        \n",
    "            train_dept_ts.iloc[1:] = smooth_sales\n",
    "            train_dept_ts = train_dept_ts.T\n",
    "            \n",
    "            nib = pd.melt(train_dept_ts, id_vars=['Date'])\n",
    "            nib[\"Dept\"] = dept\n",
    "            \n",
    "            train_svd = pd.concat([train_svd, nib], ignore_index=True)\n",
    "        except ValueError:\n",
    "            continue\n",
    "            \n",
    "    train_svd = train_svd.rename(columns={\"value\": \"Weekly_Sales\"})\n",
    "    train_svd[\"Weekly_Sales\"] = train_svd[\"Weekly_Sales\"].astype(float)\n",
    "    train_svd[\"Store\"] = train_svd[\"Store\"].astype(int)\n",
    "    \n",
    "    return train_svd\n",
    "\n",
    "def post_prediction_adjustment(test_pred, shift=1/7):\n",
    "    # Define the critical weeks\n",
    "    critical_weeks = ['2011-12-16', '2011-12-23', '2011-12-30', '2012-01-06', '2012-01-13']\n",
    "    test_pred['Date'] = pd.to_datetime(test_pred['Date'])\n",
    "    test_pred['Wk'] = test_pred['Date'].dt.isocalendar().week\n",
    "\n",
    "    # average sales for weeks 49, 50, and 51\n",
    "    avg_sales_49_51 = test_pred[test_pred['Date'].isin(['2011-12-02', '2011-12-09', '2011-12-16'])].groupby(['Store', 'Dept'])['Weekly_Pred'].mean().reset_index()\n",
    "\n",
    "    # average sales for weeks 48 and 52\n",
    "    avg_sales_48_52 = test_pred[test_pred['Date'].isin(['2011-11-25', '2011-12-30'])].groupby(['Store', 'Dept'])['Weekly_Pred'].mean().reset_index()\n",
    "\n",
    "    merged_avg = pd.merge(avg_sales_49_51, avg_sales_48_52, on=['Store', 'Dept'], how='inner', suffixes=('_49_51', '_48_52'))\n",
    "\n",
    "    # departments with sales bulge\n",
    "    bulge_depts = merged_avg[merged_avg['Weekly_Pred_49_51'] > 1.1 * merged_avg['Weekly_Pred_48_52']]\n",
    "\n",
    "    \n",
    "    for date in critical_weeks:\n",
    "        for _, row in bulge_depts.iterrows():\n",
    "            store, dept = row['Store'], row['Dept']\n",
    "            current_week_sales = test_pred[(test_pred['Date'] == date) & (test_pred['Store'] == store) & (test_pred['Dept'] == dept)]['Weekly_Pred']\n",
    "            \n",
    "            if not current_week_sales.empty:\n",
    "                test_pred.loc[(test_pred['Date'] == date) & (test_pred['Store'] == store) & (test_pred['Dept'] == dept), 'Weekly_Pred'] *= (1 - shift)\n",
    "                \n",
    "                next_week = (pd.to_datetime(date) + pd.Timedelta(weeks=1)).strftime('%Y-%m-%d')\n",
    "                test_pred.loc[(test_pred['Date'] == next_week) & (test_pred['Store'] == store) & (test_pred['Dept'] == dept), 'Weekly_Pred'] += current_week_sales.values[0] * shift\n",
    "\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f53fbc74-e09d-48fa-a16b-0894fa2a53dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3069/3069 [01:11<00:00, 43.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26489, 5) data/fold_1/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3059/3059 [01:11<00:00, 42.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23524, 5) data/fold_2/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3073/3073 [01:14<00:00, 41.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26345, 5) data/fold_3/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3088/3088 [01:18<00:00, 39.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26541, 5) data/fold_4/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3096/3096 [01:20<00:00, 38.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26815, 5) data/fold_5/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3112/3112 [01:22<00:00, 37.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23772, 5) data/fold_6/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3100/3100 [01:23<00:00, 37.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26713, 5) data/fold_7/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3089/3089 [01:26<00:00, 35.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26560, 5) data/fold_8/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3078/3078 [01:27<00:00, 35.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26579, 5) data/fold_9/mypred.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3062/3062 [01:26<00:00, 35.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23704, 5) data/fold_10/mypred.csv\n"
     ]
    }
   ],
   "source": [
    "num_folds = 10\n",
    "for fold in range(1, num_folds + 1):\n",
    "    # pre-allocate a pd to store the predictions\n",
    "    test_pred = pd.DataFrame()\n",
    "    \n",
    "    train = pd.read_csv(f'data/fold_{fold}/train.csv')\n",
    "    # Smooth data (remove noise) using PCA       \n",
    "    train = PCATransform(train, d=8)\n",
    "    \n",
    "    test = pd.read_csv(f'data/fold_{fold}/test.csv')\n",
    "\n",
    "    # Get store/dept pairs that appear in both train and test\n",
    "    train_pairs = train[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    test_pairs = test[['Store', 'Dept']].drop_duplicates(ignore_index=True)\n",
    "    unique_pairs = pd.merge(train_pairs, test_pairs, how = 'inner', on =['Store', 'Dept'])\n",
    "\n",
    "    # Create design matrix for each store/dept pair\n",
    "    train_split = unique_pairs.merge(train, on=['Store', 'Dept'], how='left')\n",
    "    train_split = preprocess(train_split)\n",
    "    y, X = patsy.dmatrices('Weekly_Sales ~ Weekly_Sales + Store + Dept + Yr  + Wk + I(Yr**2)', \n",
    "                           data = train_split, \n",
    "                           return_type='dataframe')\n",
    "    train_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "\n",
    "    test_split = unique_pairs.merge(test, on=['Store', 'Dept'], how='left')\n",
    "    test_split = preprocess(test_split)\n",
    "    y, X = patsy.dmatrices('Yr ~ Store + Dept + Yr  + Wk + I(Yr**2)', \n",
    "                           data = test_split, \n",
    "                           return_type='dataframe')\n",
    "    X['Date'] = test_split['Date']\n",
    "    test_split = dict(tuple(X.groupby(['Store', 'Dept'])))\n",
    "\n",
    "    keys = list(train_split)\n",
    "\n",
    "    # Build model for each store/dept pair\n",
    "    for key in tqdm(keys):\n",
    "        X_train = train_split[key]\n",
    "        X_test = test_split[key]\n",
    "\n",
    "        Y = X_train['Weekly_Sales']\n",
    "        X_train = X_train.drop(['Weekly_Sales','Store', 'Dept'], axis=1)\n",
    "\n",
    "        # Drop const columns\n",
    "        cols_to_drop = X_train.columns[(X_train == 0).all()]\n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "        # Drop linearly dependent columns\n",
    "        cols_to_drop = []\n",
    "        for i in range(len(X_train.columns) - 1, 1, -1):  # Start from the last column and move backward\n",
    "            col_name = X_train.columns[i]\n",
    "            # Extract the current column and all previous columns\n",
    "            tmp_Y = X_train.iloc[:, i].values\n",
    "            tmp_X = X_train.iloc[:, :i].values\n",
    "    \n",
    "            coefficients, residuals, rank, s = np.linalg.lstsq(tmp_X, tmp_Y, rcond=None)\n",
    "            if np.sum(residuals) < 1e-10:\n",
    "                    cols_to_drop.append(col_name)\n",
    "                \n",
    "        X_train = X_train.drop(columns=cols_to_drop)\n",
    "        X_test = X_test.drop(columns=cols_to_drop)\n",
    "\n",
    "        tmp_pred = X_test[['Store', 'Dept', 'Date']]\n",
    "        X_test = X_test.drop(['Store', 'Dept', 'Date'], axis=1)\n",
    "        \n",
    "        # Build model and predict\n",
    "        model = sm.OLS(Y, X_train).fit()\n",
    "        mycoef = model.params.fillna(0)\n",
    "        \n",
    "        tmp_pred['Weekly_Pred'] = np.dot(X_test, mycoef)\n",
    "        test_pred = pd.concat([test_pred, tmp_pred], ignore_index=True)\n",
    "\n",
    "    test_pred['Weekly_Pred'].fillna(0, inplace=True)\n",
    "    test_pred = post_prediction_adjustment(test_pred, shift=1/7)\n",
    "    file_path = f'data/fold_{fold}/mypred.csv'\n",
    "    print(test_pred.shape, file_path)\n",
    "    test_pred.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abb8e607-ff04-4276-ba37-1eaa1b163f95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1944.027077415397, 1362.0392914112012, 1379.8216756610193, 1525.6178760546272, 2145.015414702814, 1636.6146799855312, 1612.6082414810323, 1354.3464306177025, 1335.893209275752, 1332.118979529825]\n",
      "1562.8102876134901\n"
     ]
    }
   ],
   "source": [
    "def myeval():\n",
    "    file_path = 'data/test_with_label.csv'\n",
    "    test_with_label = pd.read_csv(file_path)\n",
    "    wae = []\n",
    "\n",
    "    num_folds = 10\n",
    "    for fold in range(1, num_folds + 1):\n",
    "        file_path = f'data/fold_{fold}/test.csv'\n",
    "        test = pd.read_csv(file_path)\n",
    "        test = test.drop(columns=['IsHoliday']).merge(test_with_label, on=['Date', 'Store', 'Dept'])\n",
    "\n",
    "        file_path = f'data/fold_{fold}/mypred.csv'\n",
    "        test_pred = pd.read_csv(file_path)\n",
    "\n",
    "        # Left join with the test data\n",
    "        new_test = test_pred.merge(test, on=['Date', 'Store', 'Dept'], how='left')\n",
    "\n",
    "        # Compute the Weighted Absolute Error\n",
    "        actuals = new_test['Weekly_Sales']\n",
    "        preds = new_test['Weekly_Pred']\n",
    "        weights = new_test['IsHoliday'].apply(lambda x: 5 if x else 1)\n",
    "        wae.append(sum(weights * abs(actuals - preds)) / sum(weights))\n",
    "\n",
    "    return wae\n",
    "\n",
    "wae = myeval() \n",
    "print(wae)\n",
    "print(np.mean(wae))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
